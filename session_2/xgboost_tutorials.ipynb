{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to XGBoost**\n",
    "\n",
    "by _Titipat Achakulvisut_\n",
    "\n",
    "In this tutorial, I use all examples and materials from [ParrotPrediction/docker-course-xgboost](https://github.com/ParrotPrediction/docker-course-xgboost). See the full slides of this presentation [here](http://kordinglab.com/lab_teaching_2016/session_2/)\n",
    "\n",
    "Thanks [Norbert](https://github.com/khozzy) to allow using his materials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install XGBoost**\n",
    "\n",
    "After install `XGBoost`, we'll print out version of `xgboost` and `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Single decision tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RoozbehFarhoudi/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "seed = 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=8, n_redundant=3, \n",
    "                           n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from IPython.display import Image\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=seed)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred  = decision_tree.predict(X_test)\n",
    "y_pred_prob  = decision_tree.predict_proba(X_test)\n",
    "\n",
    "# evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Decision Tree\n",
      "0.78\n",
      "7.59853080688\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "print(\"Single Decision Tree\")\n",
    "print(accuracy)\n",
    "print(logloss)\n",
    "print(decision_tree.tree_.node_count) # number of node created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.tree import export_graphviz\n",
    "dot_file = os.path.join(os.getcwd(), 'images', 'tree.dot')\n",
    "export_graphviz(decision_tree, out_file=dot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize decision tree**\n",
    "\n",
    "```bash\n",
    "brew install graphviz\n",
    "dot -Tpng tree.dot -o tree.png # visualize tree\n",
    "```\n",
    "\n",
    "## **AdaBoost**\n",
    "\n",
    "We'll run Adaboost classifier with 1000 stump trees (only one decision node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== AdaBoost ==\n",
      "0.78\n",
      "0.687665259874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    algorithm='SAMME',\n",
    "    n_estimators=1000,\n",
    "    random_state=seed)\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "y_pred_prob = adaboost.predict_proba(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "\n",
    "print(\"== AdaBoost ==\")\n",
    "print(accuracy)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(adaboost.estimators_))\n",
    "print(adaboost.estimators_[0]) # estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Boosted Trees**\n",
    "\n",
    "We'll construct gradient boosted tree with 1000 estimators (trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Gradient Boosting ==\n",
      "0.81\n",
      "0.480566182642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(\n",
    "    max_depth=1,\n",
    "    n_estimators=1000,\n",
    "    warm_start=False,\n",
    "    random_state=seed)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = gbc.predict(X_test)\n",
    "y_pred_prob = gbc.predict_proba(X_test)\n",
    "\n",
    "# calculate log loss\n",
    "gbc_accuracy = accuracy_score(y_test, y_pred)\n",
    "gbc_logloss = log_loss(y_test, y_pred_prob)\n",
    "\n",
    "print(\"== Gradient Boosting ==\")\n",
    "print(gbc_accuracy)\n",
    "print(gbc_logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost Standard interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('data/agaricus.txt.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':2,\n",
    "    'silent':1,\n",
    "    'eta':1\n",
    "}\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, dtrain, num_boost_round=num_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also observe performace on test dataset using `watchlist` (native interface only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\n",
      "[1]\ttest-error:0.021726\ttrain-error:0.022263\n",
      "[2]\ttest-error:0.006207\ttrain-error:0.007063\n",
      "[3]\ttest-error:0.018001\ttrain-error:0.0152\n",
      "[4]\ttest-error:0.006207\ttrain-error:0.007063\n"
     ]
    }
   ],
   "source": [
    "watchlist  = [(dtest,'test'), (dtrain,'train')]\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_prob = bst.predict(dtest)\n",
    "labels = dtest.get_label()\n",
    "preds = preds_prob > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"total %i/%i\" % (np.sum(labels == preds), len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost Scikit-learn interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.datasets import load_svmlight_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_svmlight_files(('data/agaricus.txt.train', \n",
    "                                                        'data/agaricus.txt.test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters are set like in the previous example\n",
    "- we are dealing with binary classification problem (`'objective':'binary:logistic'`),\n",
    "- we want shallow single trees with no more than 2 levels (`'max_depth':2`),\n",
    "- we don't any oupout (`'silent':1`),\n",
    "- we want algorithm to learn fast and aggressively (`'learning_rate':1`), (in naive named `eta`)\n",
    "- we want to iterate only 5 rounds (`n_estimators`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 1.0,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = XGBClassifier(**params).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_prob = bst.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"total %i/%i\" % (np.sum(labels == preds), len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spot most important features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('data/agaricus.txt.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':1,\n",
    "    'silent':1,\n",
    "    'eta':0.5\n",
    "}\n",
    "num_rounds = 5\n",
    "watchlist  = [(dtest,'test'), (dtrain,'train')]\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see which features provided the most gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst, importance_type='gain', xlabel='Gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **F-score** - sums up how many times a split was performed on each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = bst.get_fscore()\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bias and Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.learning_curve import validation_curve\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=8, n_redundant=3, \n",
    "                           n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(y, n_folds=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 1,\n",
    "    'learning_rate': 0.3,\n",
    "    'silent': 1.0\n",
    "}\n",
    "\n",
    "n_estimators_range = np.linspace(1, 200, 10).astype('int')\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    XGBClassifier(**default_params),\n",
    "    X, y,\n",
    "    param_name = 'n_estimators',\n",
    "    param_range = n_estimators_range,\n",
    "    cv=cv,\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "\n",
    "plt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\n",
    "plt.xlabel(\"number of trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.7, 1.1)\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             train_scores_mean,\n",
    "             label=\"Training score\",\n",
    "             color=\"r\")\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             test_scores_mean, \n",
    "             label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "\n",
    "plt.fill_between(n_estimators_range, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.2, color=\"r\")\n",
    "\n",
    "plt.fill_between(n_estimators_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.2, color=\"g\")\n",
    "\n",
    "plt.axhline(y=1, color='k', ls='dashed')\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = np.argmax(test_scores_mean)\n",
    "print(test_scores_mean[i], n_estimators_range[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do something > use 70 % randomly chosen and 60 % randomly chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2, # changed\n",
    "    'learning_rate': 0.3,\n",
    "    'silent': 1.0,\n",
    "    'colsample_bytree': 0.6, # added\n",
    "    'subsample': 0.7 # added\n",
    "}\n",
    "\n",
    "n_estimators_range = np.linspace(1, 200, 10).astype('int')\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    XGBClassifier(**default_params),\n",
    "    X, y,\n",
    "    param_name = 'n_estimators',\n",
    "    param_range = n_estimators_range,\n",
    "    cv=cv,\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "\n",
    "plt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\n",
    "plt.xlabel(\"number of trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.7, 1.1)\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             train_scores_mean,\n",
    "             label=\"Training score\",\n",
    "             color=\"r\")\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             test_scores_mean, \n",
    "             label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "\n",
    "plt.fill_between(n_estimators_range, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.2, color=\"r\")\n",
    "\n",
    "plt.fill_between(n_estimators_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.2, color=\"g\")\n",
    "\n",
    "plt.axhline(y=1, color='k', ls='dashed')\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = np.argmax(test_scores_mean)\n",
    "print(test_scores_mean[i], n_estimators_range[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyper parameters tuning**\n",
    "\n",
    "first we'll use `GridSearchCV` on `max_depth`, `n_estimators` and `learning_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 342\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=8, n_redundant=3, \n",
    "                           n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(y, n_folds=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'n_estimators': [5, 10, 25, 50],\n",
    "    'learning_rate': np.linspace(1e-16, 1, 3)\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_grid = GridSearchCV(\n",
    "    estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "    param_grid=params_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let's try `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_dist_grid = {\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'n_estimators': randint(1, 1001), # uniform discrete random distribution\n",
    "    'learning_rate': uniform(), # gaussian distribution\n",
    "    'subsample': uniform(), # gaussian distribution\n",
    "    'colsample_bytree': uniform() # gaussian distribution\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rs_grid = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "    param_distributions=params_dist_grid,\n",
    "    n_iter=10,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate Results**\n",
    "\n",
    "there are many `eval_metric` that you can select from (see more on slides). You can also use custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':1,\n",
    "    'silent':1,\n",
    "    'eta':0.5\n",
    "}\n",
    "num_rounds = 5\n",
    "\n",
    "dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('data/agaricus.txt.test')\n",
    "watchlist  = [(dtest,'test'), (dtrain,'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params['eval_metric'] = 'error'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params['eval_metric'] = 'logloss'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params['eval_metric'] = ['logloss', 'auc']\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def misclassified(pred_probs, dtrain):\n",
    "    labels = dtrain.get_label() # obtain true labels\n",
    "    preds = pred_probs > 0.5 # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_results = {}\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, \n",
    "                feval=misclassified, maximize=False, evals_result=e_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rounds = 10\n",
    "hist = xgb.cv(params, dtrain, num_rounds, nfold=10, metrics={'error'}, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "- by default we get a pandas data frame object (can be changed with `as_pandas` param),\n",
    "- metrics are passed as an argument (muliple values are allowed),\n",
    "- we can use own evaluation metrics (param `feval` and `maximize`),\n",
    "- we can use early stopping feature (param `early_stopping_rounds`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rounds = 1500\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(bst.best_score)\n",
    "print(bst.best_iteration)\n",
    "print(bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_v = np.random.rand(10,5)\n",
    "nan_loc = [(2,3), (0,1), (0,2), (1,0), (4,4), (7,2), (9,1)] # nan location\n",
    "data_m = np.copy(data_v)\n",
    "for (i, j) in nan_loc:\n",
    "    data_m[i, j] = np.nan\n",
    "np.random.seed(seed)\n",
    "label = np.random.randint(2, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify general training parameters\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':1,\n",
    "    'silent':1,\n",
    "    'eta':0.5\n",
    "}\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain_v = xgb.DMatrix(data_v, label=label)\n",
    "xgb.cv(params, dtrain_v, num_boost_round=num_rounds, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain_m = xgb.DMatrix(data_m, label=label, missing=np.nan) # add missing here\n",
    "xgb.cv(params, dtrain_m, num_rounds, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like it works also with missing value. Now, let's try it with scikit-learn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 1,\n",
    "    'learning_rate': 0.5,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 5\n",
    "}\n",
    "clf = XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, data_v, label, cv=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, data_m, label, cv=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods works with missing datasets. The Sklearn package by default handles data with `np.nan` as missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling imbalance dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=5,\n",
    "    n_informative=3,\n",
    "    n_classes=2,\n",
    "    weights=[.9, .1],\n",
    "    shuffle=True,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':1,\n",
    "    'silent':1,\n",
    "    'eta':1\n",
    "}\n",
    "\n",
    "num_rounds = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "y_test_preds = (bst.predict(dtest) > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_preds))\n",
    "print(precision_score(y_test, y_test_preds))\n",
    "print(recall_score(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add weight to class `1` in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.zeros(len(y_train))\n",
    "weights[y_train == 0] = 1\n",
    "weights[y_train == 1] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights) # weights added\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "y_test_preds = (bst.predict(dtest) > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_preds))\n",
    "print(precision_score(y_test, y_test_preds))\n",
    "print(recall_score(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `scale_pos_weight` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = dtrain.get_label()\n",
    "ratio = float(np.sum(train_labels == 0)) / np.sum(train_labels == 1)\n",
    "params['scale_pos_weight'] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "y_test_preds = (bst.predict(dtest) > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_preds))\n",
    "print(precision_score(y_test, y_test_preds))\n",
    "print(recall_score(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open(\"../custom.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
