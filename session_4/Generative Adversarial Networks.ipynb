{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(1, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(\n",
    "                        64, 5, 5,\n",
    "                        border_mode='same',\n",
    "                        input_shape=(1, 28, 28)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(128, 5, 5))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[0, :, :]\n",
    "    return image\n",
    "\n",
    "def normalize_and_save_images(generated_images, epoch, index, path):\n",
    "    image = np.squeeze(generated_images)\n",
    "    image = (image - image.min()) / (image.max() - image.min()) * 255.0\n",
    "    Image.fromarray(image.astype(np.uint8)).save(path+\"epoch_\"+str(epoch)+\"_batch_\"+str(index)+\".png\")\n",
    "\n",
    "def train(X_train,\n",
    "          saving_path = os.getcwd()+\"/\",\n",
    "          batch_size = 100,\n",
    "          learning_rate=0.0005, \n",
    "          momentum=0.9, \n",
    "          nesterov=True,\n",
    "          n_epochs=100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: numpy array\n",
    "        training examples\n",
    "        num_images x x_pixels x y_pixels\n",
    "    saving_path: str\n",
    "        the path that generated images are saved\n",
    "    batch_size: int\n",
    "        number of images in mini batch\n",
    "    learning_rate: float\n",
    "        learning rate\n",
    "    momemtum: float\n",
    "        momentum parameter\n",
    "    nesterov: bool\n",
    "        whether to use nesterov momentum or not\n",
    "    n_epochs: int\n",
    "        number of cycles of through the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the models\n",
    "    \n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    discriminator.trainable = True\n",
    "    discriminator_on_generator = \\\n",
    "        generator_containing_discriminator(generator, discriminator)\n",
    "        \n",
    "    # Initialize optimization object\n",
    "    optim = SGD(lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "    \n",
    "    # Compile models\n",
    "    generator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optim)\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', \n",
    "                          optimizer=optim)\n",
    "    \n",
    "    discriminator_on_generator.compile(loss='binary_crossentropy', \n",
    "                                       optimizer=optim)\n",
    "    \n",
    "    # Iterate across epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
    "        \n",
    "        # Randomize order in which training data is seen\n",
    "        select = np.random.permutation(X_train.shape[0])\n",
    "                \n",
    "\n",
    "        # Iterate many times to train discriminator\n",
    "        for index in range(int(X_train.shape[0]/batch_size)):\n",
    "            # Make the generator sample random images from noise\n",
    "            noise = np.random.uniform(low=-1, high=1, size=(batch_size, 100))\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            \n",
    "            # Take a subset of real images for this mini batch\n",
    "            batch_select = select[index * batch_size: (index + 1) * batch_size]\n",
    "            real_images = X_train[batch_select]\n",
    "\n",
    "            \n",
    "            # Produce training data for discriminator\n",
    "            X = np.concatenate((real_images, generated_images))\n",
    "            y = [1] * batch_size + [0] * batch_size\n",
    "            \n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "        \n",
    "            # Keep discriminator fixed and train generator\n",
    "            noise = np.random.uniform(low=-1, high=1, size=(batch_size, 100))\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(noise, [1] * batch_size)\n",
    "            discriminator.trainable = True\n",
    "\n",
    "            # Normalize and save generated images from every 10th mini batch\n",
    "            if index % 100 == 0:\n",
    "                normalize_and_save_images(np.expand_dims(combine_images(generated_images),axis = 0), epoch, index, saving_path)\n",
    "                print 60*\"-\" \n",
    "                print(\"d_loss : %f\" % d_loss)\n",
    "                print(\"g_loss : %f\" % g_loss)\n",
    "    #Save weights\n",
    "    generator.save_weights('generator', True)\n",
    "    discriminator.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN on mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(X_train,batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN on image 5 of mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_five = X_train[y_train == 5]\n",
    "train(X_five,batch_size = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
